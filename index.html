<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>OnlineAnySeg Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/github.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  
  <!-- For math formula -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css">
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
</head>

<body>
  
  <style>
    .video-container {
      width: 30%; /* 每个容器宽度为页面宽度的32%，根据实际情况调整 */
      display: inline-block; /* 使容器并列排列 */
      margin: 1%; /* 添加外边距 */
      vertical-align: top;
    }
  
    video {
      width: 90%; /* 视频宽度填满容器 */
      display: inline-block; /* 确保视频占满整个容器宽度 */
      margin: 0 auto; /* 视频居中 */
    }
  
    .description {
      text-align: center; /* 文字居中对齐 */
    }
    .scale-image {
        width: 80%; /* 设定特定图片的宽度 */
        height: auto; /* 保持宽高比 */
        transition: transform 0.3s ease; /* 添加动画过渡效果 */
    }
    .text-image-container {
      display: flex; /* 使用弹性盒模型布局 */
      align-items: flex-start; /* 上端对齐 */
      justify-content: center;
  }
  
  ul {
    text-align: justify; /* 确保文本左对齐 */
    list-style-position: inside; /* 列表标记与文本对齐 */
    font-size: 16px;
    list-style-type: circle; /* 使用圆形作为项目符号 */
  
  }
  
  </style>

  <!-- Header -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            
            <h1 class="title is-1 publication-title">OnlineAnySeg: Online Zero-Shot 3D Segmentation by Visual Foundation Model Guided 2D Mask Merging</h1>
            
            <!-- Paper authors -->
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://github.com/yjtang249" target="_blank">Yijie Tang</a><sup>1,*</sup>&nbsp;&nbsp;&nbsp;
                <a href="https://jzhzhang.github.io/" target="_blank">Jiazhao Zhang</a><sup>3,*</sup>&nbsp;&nbsp;&nbsp;
                <span>Yuqing Lan</span><sup>1</sup>&nbsp;&nbsp;&nbsp;
                <a href="https://www.yulanguo.cn/" target="_blank">Yulan Guo</a><sup>4</sup>&nbsp;&nbsp;&nbsp;
                <span>Dezun Dong</span><sup>1</sup>&nbsp;&nbsp;&nbsp;
                <a href="https://www.zhuchenyang.net/" target="_blank">Chenyang Zhu</a><sup>1,†</sup>
                <a href="https://kevinkaixu.net/" target="_blank">Kai Xu</a><sup>1,2,†</sup>
              </span>    
            </div>

            <!-- Institutions -->
            <div class="is-size-5 publication-authors">
              <span class="author-block"> 
                <sup>1</sup>National University of Defense Technology&nbsp;&nbsp;&nbsp;
                <sup>2</sup>Xiangjiang Laboratory&nbsp;&nbsp;&nbsp;<br>
                <sup>3</sup>CFCS, School of CS, Peking University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>4</sup>Sun Yat-sen University
                <!-- <br><small>CVPR 2025</small> -->
              </span>
              <span class="eql-cntrb">
                <small><br>
                  <sup>*</sup>Equal Contribution&nbsp;&nbsp;&nbsp;
                  <sup>†</sup>Corresponding Authors
                </small>
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                
                <!-- Paper link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2503.01309" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
                </span>

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                  <a href="static/pdfs/supplementary_material.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span> -->

                <!-- Code link -->
                <span class="link-block">
                  <a target="_blank" class="external-link button is-normal is-rounded is-light">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (soon)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                  </a>
                </span> -->
                
              </div>
            </div>

            <!-- CVPR -->
            <div class="is-size-5 publication-authors">
              <span class="author-block"> 
                CVPR 2025
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser videos-->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop has-text-centered">
        <h2 class="title is-3">Real-world Demos</h2>
        <div id="results-carousel-teaser1" class="carousel results-carousel">
  
          <div class="item item-video1">
            <video poster="" id="video1" autoplay playsinline controls muted height="100%">
              <source src="static/videos/real_world1.mp4"
              type="video/mp4">
            </video>
          </div>
          <div class="item item-video2">
            <video poster="" id="video2" autoplay playsinline controls muted height="100%">
              <source src="static/videos/real_world2.mp4"
              type="video/mp4">
            </video>
          </div>
          <div class="item item-video3">
            <video poster="" id="video3" autoplay playsinline controls muted height="100%">
              <source src="static/videos/real_world3.mp4"
              type="video/mp4">
            </video>
          </div>
        
        </div>
      </div>
    </div>
  </section>
  <!-- End teaser videos -->
  


  <!-- Contributions -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full" style="max-width: 900px;"> <!-- 调整列宽 -->
          <h2 class="title is-3 has-text-centered">Contributions</h2>
          <div class="content" style="text-align: justify; font-weight: normal;">  <!-- 去掉加粗 -->
            <ul>
              <li style="margin-bottom: 12px;"><span>OnlineAnySeg propose an efficient data structure for organizing sequential 2D masks, which can incrementally maintain the spatial associations between all the masks in real-time.</span></li>
              <li style="margin-bottom: 12px;"><span>OnlineAnySeg designs a zero-shot online mask merging strategy. By leveraging spatial overlap and multimodal similarity through collaborative filtering, this approach eliminates the dependency on training data, enabling it to maintain good performance even in incomplete scanned scenes.</span></li>
              <li><span>OnlineAnySeg performs comparably with existing offline methods and gains notable improvements over the SOTA online method on the publicly available benchmark, running at ~15 FPS.</span></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Contributions -->

  <!-- Abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Online 3D instance segmentation of a progressively reconstructed scene is both a critical and challenging task for embodied applications. 
              With the success of visual foundation models (VFMs) in the image domain, leveraging 2D priors to address 3D online segmentation has become a prominent research focus. 
              Since segmentation results provided by 2D priors often require spatial consistency to be lifted into final 3D segmentation, 
              an efficient method for identifying spatial overlap among 2D masks is essential—yet existing methods rarely achieve this in real time, 
              mainly limiting its use to offline approaches. To address this, we propose an efficient method that lifts 2D masks generated by VFMs into a unified 3D instance using a hashing technique. 
              By employing voxel hashing for efficient 3D scene querying, our approach reduces the time complexity of costly spatial overlap queries from \(O(n^2)\) to \(O(n)\). 
              Accurate spatial associations further enable 3D merging of 2D masks through simple similarity-based filtering in a zero-shot manner, 
              making our approach more robust to incomplete and noisy data. Evaluated on the ScanNet and SceneNN benchmarks, our approach achieves state-of-the-art performance in online, 
              3D instance segmentation with leading efficiency.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Abstract -->

  <!-- Overview -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container" style="max-width: 1100px;">
        <h2 class="title has-text-centered">Method Overview</h2>
        <img  src="static/images/pipeline.png" />
        <p>
          <b>The Overview of OnelineAnySeg.</b> (a) A posed RGB-D stream is input to our method sequentially. (b) A series of 2D masks are generated by
          VFM from the input color image and back-projected into 3D space, establishing associations with the VoxelHashing scene representation.
          Meanwhile, semantic and geometric features of the masks are extracted from pre-trained feature extractors and, together with mask overlap
          associations, serve as the core criteria for the Mask Merging process. (c) The final prediction of 3D instances is then output.
        </p>
      </div>
    </div>
  </section>
  <!--End Overview -->

  <!-- Visual results -->
  <section class="hero is-small">
      <div class="hero-body">
        <div class="container" style="max-width: 1100px;">
          <h2 class="title has-text-centered">Experimental Results</h2>
          <img  src="static/images/visual_results.png" />
        </div>
      </div>
  </section>
  <!--End Visual results -->
  

  <!-- Other video-->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop has-text-centered">
        <h2 class="title is-3">Results on Public Datasets</h2>
        <div id="results-carousel-teaser1" class="carousel results-carousel">
  
          <div class="item item-video16">
            <video poster="" id="video16" autoplay playsinline controls muted height="100%">
              <source src="static/videos/public_dataset1.mp4"
              type="video/mp4">
            </video>
          </div>
          <div class="item item-video17">
            <video poster="" id="video17" autoplay playsinline controls muted height="100%">
              <source src="static/videos/public_dataset2.mp4"
              type="video/mp4">
            </video>
          </div>
          <div class="item item-video17">
            <video poster="" id="video17" autoplay playsinline controls muted height="100%">
              <source src="static/videos/public_dataset3.mp4"
              type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
  </section>
  <!-- End Other video -->


  <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre>@article{tang2025onlineanyseg,
            title={OnlineAnySeg: Online Zero-Shot 3D Segmentation by Visual Foundation Model Guided 2D Mask Merging},
            author={Tang, Yijie and Zhang, Jiazhao and Lan, Yuqing and Guo, Yulan and Dong, Dezun and Zhu, Chenyang and Xu, Kai},
            journal={arXiv preprint arXiv:2503.01309},
            year={2025}
          }</pre>
      </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>
</html>
