# OnlineAnySeg: Online Zero-Shot 3D Segmentation by Visual Foundation Model Guided 2D Mask Merging
### [Project Page](https://yjtang249.github.io/OnlineAnySeg/) | [Paper](https://arxiv.org/pdf/2503.01309)

#### [Yijie Tang*](https://github.com/yjtang249), [Jiazhao Zhang*](https://jzhzhang.github.io/), Yuqing Lan, [Yulan Guo](https://www.yulanguo.cn), Dezun Dong, [Chenyang Zhu†](https://www.zhuchenyang.net), [Kai Xu†](https://kevinkaixu.net)

\*Equal Contribution, †Corresponding Authors

> Online 3D instance segmentation of a progressively reconstructed scene is both a critical and challenging task for embodied applications. With the success of visual foundation models (VFMs) in the image domain, leveraging 2D priors to address 3D online segmentation has become a prominent research focus. Since segmentation results provided by 2D priors often require spatial consistency to be lifted into final 3D segmentation, an efficient method for identifying spatial overlap among 2D masks is essential—yet existing methods rarely achieve this in real time, mainly limiting its use to offline approaches. To address this, we propose an efficient method that lifts 2D masks generated by VFMs into a unified 3D instance using a hashing technique. By employing voxel hashing for efficient 3D scene querying, our approach reduces the time complexity of costly spatial overlap queries from $O(n^2)$ to $O(n)$. Accurate spatial associations further enable 3D merging of 2D masks through simple similarity-based filtering in a zero-shot manner, making our approach more robust to incomplete and noisy data. Evaluated on the ScanNet and SceneNN benchmarks, our approach achieves state-of-the-art performance in online, 3D instance segmentation with leading efficiency.

<!-- ![image](figs/teaser.png) -->
<div align=center>
<img src=figs/overview.png width=95%/>
</div>


## TODOs

* [x] Release the main code of OnlineAnySeg.
* [x] Release the evaluation code (SOON).
* [ ] Improvement: replace the threshold-based mask merging strategy with an adaptive approach.


## 1. Data Preparation
For dataset preparation, please refer to the following documentation, which introduces how to prepare data from [ScanNet](https://kaldir.vc.in.tum.de/scannet_benchmark/), [SceneNN](https://hkust-vgd.github.io/scenenn/) as well as custom sequences.

* [Data preparation](./docs/data_prep.md)


## 2. Installation
(Step 1) Create conda environment, then install PyTorch and other dependencies.
```
conda create -n OASeg python=3.9
conda activate OASeg
pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html
pip install -r requirements.txt
```

(Step 2) Install [PyTorch3D](https://github.com/facebookresearch/pytorch3d)
```
pip install "git+https://github.com/facebookresearch/pytorch3d.git"
```
or install from a local clone
```
git clone https://github.com/facebookresearch/pytorch3d.git
cd pytorch3d && pip install -e .
```

(Step 3) CropFormer

The official installation of Cropformer is composed of two steps: installing detectron2 and then Cropformer. Following  the installation steps in MaskClustering, the two steps are combined into the following scripts. If you have any problems, please refer to the installation guide in [MaskClustering](https://github.com/PKU-EPIC/MaskClustering) or the original [Cropformer](https://github.com/qqlu/Entity/blob/main/Entityv2/CropFormer/INSTALL.md) installation guide.
```bash
cd third_party
git clone git@github.com:facebookresearch/detectron2.git
cd detectron2
pip install -e .

cd ../
git clone git@github.com:qqlu/Entity.git
cp -r Entity/Entityv2/CropFormer detectron2/projects
cd detectron2/projects/CropFormer/entity_api/PythonAPI
make

cd ../..
cd mask2former/modeling/pixel_decoder/ops
sh make.sh
pip install -U openmim
mim install mmcv
```
Then change directory to the root of this repository, and copy additional scripts into cropformer for processing data sequences.
```
cd <OnlineAnySeg_root>
cp scripts/mask_predict/* third_party/detectron2/projects/CropFormer/demo_cropformer
```
Finally, download the [CropFormer checkpoint](https://huggingface.co/datasets/qqlu1992/Adobe_EntitySeg/blob/main/CropFormer_model/Entity_Segmentation/Mask2Former_hornet_3x/Mask2Former_hornet_3x_576d0b.pth) and [CLIP checkpoint](https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K/blob/main/open_clip_pytorch_model.bin) and place them into `./models` directory at the root of the repository.


(Step 4) FCGF

In our method, we use FCGF to extract geometric features from 3D point cloud. The [FCGF](https://github.com/chrischoy/FCGF) repository is already placed at 'third_party/'. If you have any problems, please follow the [FCGF](https://github.com/chrischoy/FCGF) installation guide.

Install MinkowskiEngine:
```
pip install git+https://github.com/NVIDIA/MinkowskiEngine.git
```
If it is not installed successfully, you can try to install it from a local clone:
```
cd third_party
git clone https://github.com/NVIDIA/MinkowskiEngine.git
cd MinkowskiEngine
python setup.py install --blas_include_dirs=${CONDA_PREFIX}/include --blas=openblas
```


## 3. Running
### Run a given sequence (demo)
In this section, we introduce how to run a given sequence (ScanNet, SceneNN or custom sequence).

(Step 1) To segment a given sequence, and save detected 2D masks as well as corresponding CLIP features to the specified dir, you can run the following command:
```
# python third_party/detectron2/projects/CropFormer/demo_cropformer/mask_predict_single_seq_w_semantic.py \
    --root <root_dir_of_dataset> \
    --seq_name <scene_id> \
    --output_root <seg_output_dir_of_dataset> \
    --pretrained_path <CLIP_checkpoint_path> \
    --opts MODEL.WEIGHTS <Cropformer_checkpoint_path>
```
For example, if the given sequence is "scene0011_00" in ScanNet, the command to run is:
```
# python third_party/detectron2/projects/CropFormer/demo_cropformer/mask_predict_single_seq_w_semantic.py \
    --root ./data/scannet \
    --seq_name scene0011_00 \
    --output_root ./data/scannet/seg_result \
    --pretrained_path ./models/open_clip_pytorch_model.bin \
    --opts MODEL.WEIGHTS ./models/Mask2Former_hornet_3x_576d0b.pth
```

<details>
  <summary>[Example for a SceneNN sequence (click to expand)]</summary>

  For example, if the given sequence is "011" in SceneNN, the command to run is:

```
# python third_party/detectron2/projects/CropFormer/demo_cropformer/mask_predict_single_seq_w_semantic.py \
   --root ./data/SceneNN \
   --seq_name 011 \
   --output_root ./data/SceneNN/seg_result \
   --image_path_pattern image/* \
   --seg_interval 20 \
   --pretrained_path ./models/open_clip_pytorch_model.bin \
   --opts MODEL.WEIGHTS ./models/Mask2Former_hornet_3x_576d0b.pth

```
</details>

<details>
  <summary>[Example for a custom data sequence (click to expand)]</summary>

  For example, if the given sequence is "My_sequence1" in "My_dataset", the command to run is:

```
# python third_party/detectron2/projects/CropFormer/demo_cropformer/mask_predict_single_seq_w_semantic.py \
   --root ./data/My_dataset \
   --seq_name My_sequence1 \
   --output_root ./data/My_dataset/seg_result \
   --image_path_pattern color/* \
   --seg_interval 20 \
   --pretrained_path ./models/open_clip_pytorch_model.bin \
   --opts MODEL.WEIGHTS ./models/Mask2Former_hornet_3x_576d0b.pth

```
</details>

<br>

After running the command, you can find the following directory structure in `<seg_output_dir_of_dataset>/<scene_id>`:
```
<seg_output_dir_of_dataset>
    ├── <scene_id>
         ├── mask               # segmented 2D masks of each frame
         ├── mask_color         # segmented 2D masks of each frame (only for visualization)
         └── mask_embeddings    # extracted CLIP features of the 2D masks
```
As long as the segmentation results follow the directory structure above, the following steps will work seamlessly for any dataset.

(Step 2) process the RGB-D sequence:
```
# python main.py -c <config_file> --seq_name <scene_id> -d <sequence_dir> -i <seg_sequence_dir> -o <output_dir>
```

For example, if the given sequence is "scene0011_00" in ScanNet dataset, the running command is:
```
# python main.py -c config/scannet_cropformer.yaml --seq_name scene0011_00 \
    -d ./data/scannet/scene0011_00/frames \
    -i ./data/scannet/seg_result/scene0011_00 \
    -o ./output/scannet
```

<details>
  <summary>[Example for running a SceneNN sequence (click to expand)]</summary>

  For example, if the given sequence is "011" in SceneNN dataset, the command is:

```
# python main.py -c config/sceneNN_cropformer.yaml --seq_name 011 \
    -d ./data/SceneNN/011 \
    -i ./data/SceneNN/seg_result/011 \
    -o ./output/SceneNN
```
</details>

<details>
  <summary>[Example for running a custom sequence (click to expand)]</summary>

  For example, if the given sequence is "My_sequence1" in "My_dataset", the command is:

```
# python main.py -c config/mydataset_cropformer.yaml --seq_name My_sequence1 \
    -d ./data/My_dataset/My_sequence1 \
    -i ./data/My_dataset/seg_result/My_sequence1 \
    -o ./output/My_dataset
```
</details>

<br>

When finished, the 3D reconstruction and instance segmentation output of this sequence will be found at `./output/` by default (you can also add "--output_dir" in running command to specify output directory).


### Run all sequences in the dataset
TODO

## 4. Evaluation
#### 4.1 All sequences
The command to run evaluation code is:
```
# python eval/evaluate_seqs.py --result_dir <output_dir> \
    --gt_dir <gt_dataset_dir> \
    --gt_pc_pattern <gt_pointcloud_file_pattern> \
    --gt_seg_dir <gt_dataset_seg_dir> \
    --gt_seg_pattern <gt_seg_file_pattern>
```

For example, to evaluate ScanNet dataset, the running command is:
```
# python eval/evaluate_seqs.py --result_dir ./output/scannet \
    --gt_dir XXX/scannet \
    --gt_pc_pattern %s/%s_vh_clean_2.ply \
    --gt_seg_dir ./eval/scannet200/validation \
    --gt_seg_pattern %s.txt
```

<details>
  <summary>[Example for evaluating SceneNN sequences (click to expand)]</summary>

  For example, to evaluate SceneNN dataset, the running command is:

```
# python eval/evaluate_seqs.py --result_dir ./output/SceneNN \
    --gt_dir XXX/SceneNN \
    --gt_pc_pattern %s/%s.ply \
    --gt_seg_dir ./eval/sceneNN \
    --gt_seg_pattern %s.txt
```
</details>

#### 4.2 Selected sequences
If you want to elvaluate one or more selected sequences, you can easily add `--seq_name`, and `<selected_sccene_ids>` should be split by comma:
```
# python eval/evaluate_seqs.py --result_dir <output_dir> \
    --seq_name <selected_sccene_ids> \
    --gt_dir <gt_dataset_dir> \
    --gt_pc_pattern <gt_pointcloud_file_pattern> \
    --gt_seg_dir <gt_dataset_seg_dir> \
    --gt_seg_pattern <gt_seg_file_pattern>
```

For example, to evaluate "scene0011_00" and "scene0015_00" in ScanNet dataset, the running command is:
```
# python eval/evaluate_seqs.py --result_dir ./output/scannet \
    --seq_name scene0011_00,scene0015_00 \
    --gt_dir XXX/scannet \
    --gt_pc_pattern %s/%s_vh_clean_2.ply \
    --gt_seg_dir ./eval/scannet200/validation \
    --gt_seg_pattern %s.txt
```


## 5. Acknowledgement
Parts of the code are modified from [MaskClustering](https://github.com/PKU-EPIC/MaskClustering) and [OpenFusion](https://github.com/UARK-AICV/OpenFusion). Thanks to the original authors.


## 6. Citation
If you find our code or paper useful, please cite
```
@article{tang2025onlineanyseg,
  title={OnlineAnySeg: Online Zero-Shot 3D Segmentation by Visual Foundation Model Guided 2D Mask Merging},
  author={Tang, Yijie and Zhang, Jiazhao and Lan, Yuqing and Guo, Yulan and Dong, Dezun and Zhu, Chenyang and Xu, Kai},
  journal={arXiv preprint arXiv:2503.01309},
  year={2025}
}
```
